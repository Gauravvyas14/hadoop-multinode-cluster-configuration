##########################################################
## HADOOP MULTINODE CLUSTER SETUP                        #
##  ISSUES                                               #
##  #   1. edit /etc/hosts file                          #
##  #   2.  password less SSH                            #
##  #   3.  Installing jdk7                              #
##  #   4.  Installing and configuring hadoop-2.6.0      #
##########################################################

## DO THE BELOW IN EACH NODE

##  #   1.   edit /etc/hosts file
# find the ip address of each node and enter them in to each nodes /etc/hosts file

sudo gedit /etc/hosts
#   save and close the gedit

##  #   2.  password less SSH

su - root

service sshd restart

# exit from root
exit

# generate rsa keys
ssh-keygen -t rsa

# just keep pressing enter, when prompt for the enter passphrase, leave it as it is and press enter
        ##########################################
        
#############################       
## DO THE BELOW IN NAMENODE #
#############################

#   copy the public key

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


#   copy the namenode public key to all the nodes

scp ~/.ssh/id_rsa.pub hadoop@<IP address of node>:/home/hadoop/.ssh/ authorized_keys


##  Change the Permission to authorized_keys. (Do the step in all the nodes)

chmod 755 ~/.ssh/authorized_keys


##  #   3.  Installing jdk7                              

wget http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz

tar -xvf jdk-7u79-linux-x64.tar.gz


##  #   4.  Installing and configuring hadoop-2.6.0
#  download the hadoop and then extract tar ball

tar -xvf hadoop-2.6.0.tar.gz

# add paths to jdk and hadoop to your bashrc

sudo gedit ~/.bashrc

# add below paths to your .bashrc

export HADOOP_HOME=< Path to your Hadoop-2.6.0 directory>
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export JAVA_HOME=<Path to your Java Directory>
export PATH=$PATH: $JAVA_HOME/bin: $HADOOP_HOME/bin: $HADOOP_HOME/sbin

# save and close gedit 


source ~/.bashrc

# check if you have done everything correct

java -version
# it will give you your jdk's version 

hadoop version

# will give the version of your hadoop

##  WE ARE NOT DONE YET, WE HAVE JUST FINISHED THE INSTALLATIONS, NOW WE NEED TO CONFIGURE OUR CLUSTER
##  Edit the Hadoop Configuration files.

cd $HADOOP_HOME
cd /etc/hadoop

#########################
#
## edit core-site.xml   #
#
#########################

sudo gedit core-site.xml

# add the below in to the file
<configuration>
<property>
<name>fs.defaultFS</name>
<value>hdfs://nn.cluster.com:9000</value>
</property>
</configuration>

#########################
#                       #
## edit hdfs-site.xml   #
#                       #
#########################

cd
mkdir namenode
chmod 755 namenode

cd $HADOOP_HOME/etc/hadoop
sudo gedit hdfs-site.xml

# add the below properties in to the file

<configuration>
<property>
<name>dfs.namenode.name.dir</name>
<value>/home/hadoop/namenode</value>
</property>

<property>
<name>dfs.replication</name>
<value>1</value>
</property>

<property>
<name>dfs.permission.enabled</name>
<value>false</value>
</property>
</configuration>

#########################
#                       #
## edit mapred-site.xml #
#                       #
#########################

cp mapred-site.xml.template mapred-site.xml

sudo gedit mapred-site.xml

# add the below properties in to the file
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>

</configuration>


#########################
#                       #
## edit yarn-site.xml   #
#                       #
#########################
<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>rm.cluster.com:9001</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.address</name>
<value>rm.cluster.com:9002</value>
</property>
<property>
<name>yarn.resourcemanager.address</name>
<value>rm.cluster.com:9003</value>
</property>
</configuration>


#########################
#                       #
## edit slaves file     #
#                       #
#########################

sudo gedit slaves
# add IP addresses of each node in it, save and close


################################################################################
#   
##  Copy the Hadoop, Java and .bashrc file to all the nodes from the namenode   #
#
#################################################################################

scp -r hadoop-2.6.0/ hadoop@192.168.1.6:/home/hadoop/
scp -r jdk1.7.0_79/ hadoop@192.168.1.6:/home/hadoop/
scp ~/.bashrc hadoop@192.168.1.6:/home/hadoop/

## do source ~/.bashrc in each node

## on all the datanodes do the below steps

mkdir datanode
chmod 755 datanode

cd $HADOOP_HOME/etc/hadoop

sudo gedit hdfs-site.xml

<configuration>
<property>
<name>dfs.datanode.name.dir</name>
<value>/home/hadoop/datanode</value>
</property>

<property>
<name>dfs.replication</name>
<value>1</value>
</property>

<property>
<name>dfs.permission.enabled</name>
<value>false</value>
</property>
</configuration>

## open the slaves file and keep the IP address of that node only, save and close

## format the namenode

hadoop namenode -format

## start all the hadoop daemons

start-dfs.sh
start-yarn.sh

## now you can run jps to check if hadoop is running or not